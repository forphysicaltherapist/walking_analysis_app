import os
import streamlit as st
import cv2
import numpy as np
import tempfile
import mediapipe as mp
import pandas as pd
import plotly.express as px
from supabase import create_client, Client
from openai import OpenAI
import json

# âœ… ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

# âœ… Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# âœ… OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… Mediapipe ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

# âœ… Webã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸš¶â€â™‚ï¸ æ­©è¡Œåˆ†æã‚¢ãƒ—ãƒª")
st.write("å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨æ­©è¡Œã‚’è§£æã—ã¾ã™ï¼")

# âœ… å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("æ­©è¡Œå‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["mp4", "mov"])

if uploaded_file:
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    temp_file.write(uploaded_file.read())

    cap = cv2.VideoCapture(temp_file.name)

    # âœ… ä¿å­˜ç”¨ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    output_video_path = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4").name
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

    if st.button("æ­©è¡Œè§£æã‚’é–‹å§‹"):
        st.write("è§£æä¸­...")

        joint_data = []

        # âœ… å–å¾—ã™ã‚‹é–¢ç¯€
        JOINTS = {
            "LEFT_HIP": mp_pose.PoseLandmark.LEFT_HIP,
            "RIGHT_HIP": mp_pose.PoseLandmark.RIGHT_HIP,
            "LEFT_KNEE": mp_pose.PoseLandmark.LEFT_KNEE,
            "RIGHT_KNEE": mp_pose.PoseLandmark.RIGHT_KNEE,
            "LEFT_ANKLE": mp_pose.PoseLandmark.LEFT_ANKLE,
            "RIGHT_ANKLE": mp_pose.PoseLandmark.RIGHT_ANKLE
        }

        # âœ… Pose ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = pose.process(image)

                if results.pose_landmarks:
                    frame_data = {"Time (s)": cap.get(cv2.CAP_PROP_POS_FRAMES) / cap.get(cv2.CAP_PROP_FPS)}
                    for joint_name, joint_id in JOINTS.items():
                        landmark = results.pose_landmarks.landmark[joint_id]
                        frame_data[f"{joint_name}_Y"] = landmark.y

                    joint_data.append(frame_data)

                    # âœ… é–¢ç¯€ãƒãƒ¼ã‚«ãƒ¼ã‚’æç”»
                    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

                # âœ… ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‹•ç”»ã«ä¿å­˜
                out.write(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

        cap.release()
        out.release()

        df = pd.DataFrame(joint_data)
        st.write("âœ… è§£æå®Œäº†ï¼")

        # âœ… æ­©è¡Œãƒãƒ©ãƒ³ã‚¹ã®ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
        fig = px.line(df, x="Time (s)", y=["LEFT_KNEE_Y", "RIGHT_KNEE_Y", "LEFT_ANKLE_Y", "RIGHT_ANKLE_Y"],
                      title="æ­©è¡Œãƒãƒ©ãƒ³ã‚¹ã®å¤‰åŒ–", labels={"value": "é–¢ç¯€ã®é«˜ã•", "variable": "é–¢ç¯€"})
        st.plotly_chart(fig)

        # âœ… è§£æå‹•ç”»ã‚’è¡¨ç¤º
        st.subheader("ğŸ¥ è§£æçµæœã®å‹•ç”»")
        st.video(output_video_path)

        # âœ… è§£æå‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(output_video_path, "rb") as file:
            st.download_button("ğŸ“¥ è§£æå‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", file, file_name="walking_analysis.mp4", mime="video/mp4")

        # âœ… æ­©è¡Œã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        def calculate_gait_scores(df):
            scores = {}
            scores["Stability Score"] = max(0, 100 - (df["LEFT_KNEE_Y"].std() + df["RIGHT_KNEE_Y"].std()) * 50)
            step_intervals = np.diff(df["Time (s)"])
            scores["Gait Rhythm Score"] = max(0, 100 - np.std(step_intervals) * 500)
            scores["Symmetry Score"] = max(0, 100 - np.mean(np.abs(df["LEFT_KNEE_Y"] - df["RIGHT_KNEE_Y"])) * 500)
            return scores

        scores = calculate_gait_scores(df)
        st.metric(label="æ­©è¡Œå®‰å®šåº¦ã‚¹ã‚³ã‚¢", value=f"{scores['Stability Score']:.1f} / 100")

        # âœ… AI ã«è§£æãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ã—ã€è§£èª¬ã‚’å–å¾—
        def generate_ai_analysis(scores_json):
            prompt = f"""
            ã‚ãªãŸã¯æ­©è¡Œè§£æã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®è§£æçµæœã‚’ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ï¼š
            {json.dumps(scores_json, indent=2, ensure_ascii=False)}
            ã©ã®ã‚¹ã‚³ã‚¢ãŒè‰¯ãã€ã©ã®ã‚¹ã‚³ã‚¢ãŒæ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
            """

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯æ­©è¡Œè§£æã®å°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )

            return response.choices[0].message.content

        ai_analysis = generate_ai_analysis(scores)

        st.subheader("ğŸ“– AI ã«ã‚ˆã‚‹è§£æè§£èª¬")
        st.write(ai_analysis)

        # âœ… Supabase ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        supabase.table("walking_analysis").insert({
            "scores": json.dumps(scores),
            "ai_analysis": ai_analysis
        }).execute()

        st.success("ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¦ãƒ‰ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

